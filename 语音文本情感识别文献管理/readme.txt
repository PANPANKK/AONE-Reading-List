###语音情感识别综述_陶建华
本文从语音情感识别在国内外发展历史以及开展的一系列会议、期刊和竞赛入手，分别从6个方面对语音情感识别的研究现状进行了梳理与归纳。1.对情感表达从离散、维度模型进行了阐述；2.针对现有的情感数据库进行了统计与总结；3.回
顾了近20年部分代表性语音情感识别发展历程，并分别阐述了基于人工设计的语音情感特征的情感识别技术和基于端到端的语音情感识别技术；4.总结了近几年的语音情感识别性能，尤其是近两年在语音领域的重要会议和期刊上的语音情感
识别相关工作；5.介绍了语音情感识别在驾驶、智能交互领域、医疗健康，安全等领域的应用；6.总结与阐述了语音情感识别领域仍面临的挑战与未来发展方向。

###Liu22Group 
在多模态设置中，不同模态之间的时间对齐尚未得到充分研究。本文提出了一种名为门控双向对齐网络（GBAN）的新模型，该模型由一个基于注意力的双向对齐网络组成，通过LSTM隐藏状态明确捕捉语音和文本之间的对齐关系，以及一个
新颖的组门控融合（GGF）层来整合不同模态的表示。

###Li22Context-aware
提出了一种上下文感知多模态融合框架，包括一个基于上下文感知SSL的特征提取器和基于Transformer的音频-文本融合范式。首先使用预训练的WavLM和BERT模型，分别将原始音频和文本输入编码为帧级或标记级嵌入。然后，提取的高级
特征通过挤压和激励（S&E）模块校准和压缩，接着是一个全连接（FC）层和层归一化，这将两种模态映射到相同的空间。之后，采用一个上下文感知卷积模块来增强文本模态的目标话语的上下文信息。然后，我们提出了一种新颖的多模态
Transformer（MMT）融合模块来整合上述特征。MMT的关键组件是基于方向对跨模态注意力的Transformer（CMT），允许具有不同时间步的模态之间的交互。在融合模块之后，集成输出被送入情感分类器，取得了SOTA效果。

###Li21Learning（有代码）
考虑到情感表达复杂、多模态且细粒度。我们提出了一种新颖的多模态深度学习方法，用于从现实生活中的语音中进行细粒度的情感识别。我们设计了一种时间对齐均值-最大池化机制，以捕捉每个话语中隐含的细微和细粒度的情感。
此外，我们提出了一种跨模态激励模块，对跨模态嵌入进行样本特定的调整，并通过其对齐的另一模态的潜在特征自适应地重新校准相应的值。我们在两个著名的现实世界语音情感识别数据集上评估了我们提出的模型。结果表明，我们的方法在
多模态语音话语的预测任务上表现优越，并且在预测准确率方面优于多种基线方法。此外，我们进行了详细的消融研究，表明我们的时间对齐均-最大池化机制和跨模态激励显著贡献于令人满意的结果。
为了鼓励研究的可重复性，我们将代码公开在：https://github.com/tal-ai/FG_CME.git。

###Latiff20Unsupervised
SER系统的性能通常会因训练数据和测试数据分布的差异而下降。当训练数据和测试数据属于不同语言时，这些差异变得更加明显，导致验证分数和测试分数之间存在显著的性能差距。构建更为稳健的模型以适应实际应用中的SER系统是至关重要的。
本文提出了一种基于生成对抗网络（GAN）的多语言SER模型。我们选择使用GAN的原因在于其在学习数据分布方面取得的巨大成功。所提出的模型设计成能够学习语言不变的表示，而无需目标语言的数据标签。我们在四个不同语言的情感数据集上
评估了我们提出的模型，包括一个乌尔都语数据集，以纳入难以找到标注数据且主流社区研究较少的替代语言。我们的结果表明，所提出的模型可以显著提高所有考虑的数据集的基线跨语言SER性能，包括无需任何标签的非主流乌尔都语数据。

###Fan23Adaptive
当前，许多情感识别方法专注于域不变表示学习，这会丧失域特定的知识，导致跨域的语音情感识别效果不佳。本文提出了一种自适应域感知表示学习方法，该方法利用领域知识提取域感知特征。
所提出的方法在频率上应用注意力模型，将领域知识嵌入情感表示空间。实验表明，我们的方法在IEMOCAP数据集上的表现达到了最先进的水平，在相同实验条件下加权准确率（WA）为73.02%，未加权准确率（UA）为65.86%。

###Chen22KEY-SPARSE
使用多模态输入可以进一步提高性能，因为使用了更多的情感信息。然而，这些多模态信息中，只有一小部分是关于情感的。冗余信息会成为噪声，限制系统性能。在本文中，我们提出了KS-Transformer，使用了一种新颖的关键稀疏注意力机制用于
语音情感识别。只有与情感相关的语音帧或文本中的词语会被考虑并分配注意力权重。基于KS-Transformer，我们进一步提出了CCAB来融合不同的模态并实现深度交互。在IEMOCAP和LSSED数据集上的实验结果证明了KS-Transformer和CCAB的有
效性。

###CHEAVD数据集-清华自动化实验室
CASIA中文自然情感视听数据库（CHEAVD），包含140分钟从电影、电视剧和脱口秀中提取的情感片段，238位年龄从儿童到老年人的说话者构成了广泛的说话者多样性。总共有26种非原型情感状态，包括基本的六种情感，由四位母语者进行标注。
与其他现有情感数据库相比，我们提供了多情感标签和虚假/抑制情感标签。该数据库是首个处理多模态和自然情感的大规模中文自然情感语料库，并免费供研究使用。我们在此语料库上使用长短期记忆循环神经网络（LSTM-RNN）进行了自动情感识别。
实验表明，在六种主要情感状态下，可以实现56%的平均准确率。

###Latif20Deep
我们提出了一种更深层的神经网络架构，其中融合了DenseNet、LSTM和Highway Network，以学习强大的判别特征，这些特征对噪声具有较强的鲁棒性。我们还提出了在我们的网络架构中进行数据增强，以进一步提高鲁棒性。
我们对该架构结合数据增强在以下三种情况下进行了全面评估：(1) 噪声，(2) 对抗攻击，(3) 跨语料库设置。我们在广泛使用的IEMOCAP和MSP-IMPROV数据集上的评估显示，与现有研究和最先进模型相比，取得了有前途的结果。

###Rizos20STARGAN
许多人类具有在保持词汇结构不变的情况下操纵其情感表达的能力[19]。在这项研究中，我们利用StarGAN将真实的情感语音样本转换为不同的目标情感。通过这种方式，我们没有在信号上应用计算扰动（例如，如[20]中提出的或在[5]中添加抖动），而
是利用训练数据集中可用的情感建模信息，以尽可能保留语音内容和表达方式为原样，但改变所传达的情感。我们非常谨慎地评估生成样本的质量，并进行双重评估：a) 我们在多类情感计算任务中使用生成的样本进行数据增强；b) 我们对一组生成的样
本进行人类评估，以确定StarGAN是否真正学习了在样本中注入情感，而不是仅仅学会欺骗分类器（例如，由于模式崩溃）。为了保证可重复性，我们将在项目接受后，在项目的GitHub页面上提供所提出方法的实现，以确保原创性。
复现代码地址：https://github.com/glam-imperial/EmotionalConversionStarGAN


###SPEECH EMOTION RECOGNITION USING SEMANTIC INFORMATION
我们提出了一个使用音频和文本信息进行语音情感识别的训练框架。特别是，我们使用Word2Vec和Speech2Vec模型，并对其嵌入空间进行对齐，以仅使用语音信号进行准确的语义特征提取。我们使用一种新颖的注意力融合策略将语义和副语言特征结
合起来，该策略首先按情感维度分离信息，然后使用注意力进行结合。所提出的模型在SEWA数据集上进行了评估，并在效价和喜欢维度上与提交给AVEC 2017挑战赛的最佳论文相比，产生了最先进的结果。
代码地址：https://github.com/glam-imperial/semantic_speech_emotion_recognition?tab=readme-ov-file
SEWA数据集：使用了AVEC 2017挑战赛中使用的“野外情感分析”（SEWA）数据集。该数据集由通过网络摄像头和麦克风捕捉的32对（即64名参与者）的“野外”视听录音组成，这些参与者观看了一个90秒的商业广告，并与他们的伙伴讨论了最多3分钟。
该数据集提供了三种模态，即音频、视觉和文本，对三种情感维度进行标注：唤醒度、效价和喜欢。数据集分为三个部分：训练集（17对）、开发集（7对）和测试集（8对），并由6名德语标注员（3名女性，3名男性）进行标注。

###Upadhyay23Learning
不同的评价者对同一语音样本的情感感知可能会有所不同，这导致标注的情感标签存在主观性和模糊性。传统的SER模型通常采用所有评价者评分的平均值或共识标签进行训练，这可能会忽略不同评价者之间的主观差异。
本文通过聚类技术整合了评价者的主观性，形成感知一致性集群（PCC），将复杂的主观性问题简化为多数和少数评价者的感知聚类。
提出了一种基于评分者感知一致性的聚类方法，将评分者分为多数（MajP）和少数（MinP）两个群体，并基于此形成扩展的标签空间。
模型构建：构建了一个名为Rater Perception Coherency (RPC)-based SER的模型，该模型结合了多数和少数评分者的感知以及基于共识的情感分类器。
实验验证：在IEMOCAP和MSP-Podcast语料库上评估了提出的方法，考虑了固定和可变评分者的不同场景。
结果分析：通过实验，作者发现提出的RPC-based SER模型在UAR（Unweighted Average Recall）上分别比IEMOCAP和MSP-Podcast上的单任务模型提高了3.39%和2.03%。
代码地址：https://github.com/tal-ai/FG_CME


###Zhou23Emotion
在传统的情感语音转换（EVC）中，情感通常被视为离散的类别，忽略了语音还传达了听众可以感知到的不同强度的情感。在本文中，我们旨在明确表征和控制情感的强度。我们提出将说话者风格从语言内容中解耦，并将说话者风格编码为一个连续空间中的
风格嵌入，这构成了情感嵌入的原型。我们进一步从情感标注数据库中学习实际的情感编码器，并研究使用相对属性来表示细粒度的情感强度。为了确保情感的可理解性，我们在EVC网络的训练中加入了情感分类损失和情感嵌入相似性损失。正如预期的那样，
所提出的网络能够控制输出语音中的细粒度情感强度。通过客观和主观评估，我们验证了所提出网络在情感表达和情感强度控制方面的有效性。

代码地址（https://github.com/KunZhou9646/Emovox）

###Kreuk22Textless  meta Ai团队
在这篇论文中，研究人员把情感转换的问题作为一项口语翻译任务，将语音分解成离散的、不相干的，由内容单元、音调（f0）、说话人和情绪组成的学习表征。
模型先通过将内容单元翻译成目标情感来修改语音内容，然后根据这些单元来预测声音特征，最后通过将预测的表征送入一个神经声码器来生成语音波形。这种范式使得模型不止能发现信号的频谱和参数变化，还可以对非语言发声进行建模，如插入笑声、消除哈欠等。
比如在一个包含五种情绪表达方式（中立、愤怒、娱乐、困倦或厌恶）的情绪转换任务中，模型需要根据输入音频转换到目标情绪，可以看到整个流程就相当于是一个端到端的序列翻译问题，所以插入、删除、替换一些非语言的音频信号来转换情感就会更容易。
论文在客观上和主观上证明了所提出的方法在感知情感和音频质量方面优于基线。
代码地址：https://github.com/facebookresearch/fairseq/tree/main/examples/emotion_conversion





