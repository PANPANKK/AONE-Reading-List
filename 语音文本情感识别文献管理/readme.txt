###语音情感识别综述_陶建华
本文从语音情感识别在国内外发展历史以及开展的一系列会议、期刊和竞赛入手，分别从6个方面对语音情感识别的研究现状进行了梳理与归纳。1.对情感表达从离散、维度模型进行了阐述；2.针对现有的情感数据库进行了统计与总结；3.回
顾了近20年部分代表性语音情感识别发展历程，并分别阐述了基于人工设计的语音情感特征的情感识别技术和基于端到端的语音情感识别技术；4.总结了近几年的语音情感识别性能，尤其是近两年在语音领域的重要会议和期刊上的语音情感
识别相关工作；5.介绍了语音情感识别在驾驶、智能交互领域、医疗健康，安全等领域的应用；6.总结与阐述了语音情感识别领域仍面临的挑战与未来发展方向。

###Liu22Group 
在多模态设置中，不同模态之间的时间对齐尚未得到充分研究。本文提出了一种名为门控双向对齐网络（GBAN）的新模型，该模型由一个基于注意力的双向对齐网络组成，通过LSTM隐藏状态明确捕捉语音和文本之间的对齐关系，以及一个
新颖的组门控融合（GGF）层来整合不同模态的表示。

###Li22Context-aware
提出了一种上下文感知多模态融合框架，包括一个基于上下文感知SSL的特征提取器和基于Transformer的音频-文本融合范式。首先使用预训练的WavLM和BERT模型，分别将原始音频和文本输入编码为帧级或标记级嵌入。然后，提取的高级
特征通过挤压和激励（S&E）模块校准和压缩，接着是一个全连接（FC）层和层归一化，这将两种模态映射到相同的空间。之后，采用一个上下文感知卷积模块来增强文本模态的目标话语的上下文信息。然后，我们提出了一种新颖的多模态
Transformer（MMT）融合模块来整合上述特征。MMT的关键组件是基于方向对跨模态注意力的Transformer（CMT），允许具有不同时间步的模态之间的交互。在融合模块之后，集成输出被送入情感分类器，取得了SOTA效果。

###Li21Learning（有代码）
考虑到情感表达复杂、多模态且细粒度。我们提出了一种新颖的多模态深度学习方法，用于从现实生活中的语音中进行细粒度的情感识别。我们设计了一种时间对齐均值-最大池化机制，以捕捉每个话语中隐含的细微和细粒度的情感。
此外，我们提出了一种跨模态激励模块，对跨模态嵌入进行样本特定的调整，并通过其对齐的另一模态的潜在特征自适应地重新校准相应的值。我们在两个著名的现实世界语音情感识别数据集上评估了我们提出的模型。结果表明，我们的方法在
多模态语音话语的预测任务上表现优越，并且在预测准确率方面优于多种基线方法。此外，我们进行了详细的消融研究，表明我们的时间对齐均-最大池化机制和跨模态激励显著贡献于令人满意的结果。
为了鼓励研究的可重复性，我们将代码公开在：https://github.com/tal-ai/FG_CME.git。

###Latiff20Unsupervised
SER系统的性能通常会因训练数据和测试数据分布的差异而下降。当训练数据和测试数据属于不同语言时，这些差异变得更加明显，导致验证分数和测试分数之间存在显著的性能差距。构建更为稳健的模型以适应实际应用中的SER系统是至关重要的。
本文提出了一种基于生成对抗网络（GAN）的多语言SER模型。我们选择使用GAN的原因在于其在学习数据分布方面取得的巨大成功。所提出的模型设计成能够学习语言不变的表示，而无需目标语言的数据标签。我们在四个不同语言的情感数据集上
评估了我们提出的模型，包括一个乌尔都语数据集，以纳入难以找到标注数据且主流社区研究较少的替代语言。我们的结果表明，所提出的模型可以显著提高所有考虑的数据集的基线跨语言SER性能，包括无需任何标签的非主流乌尔都语数据。

###Fan23Adaptive
当前，许多情感识别方法专注于域不变表示学习，这会丧失域特定的知识，导致跨域的语音情感识别效果不佳。本文提出了一种自适应域感知表示学习方法，该方法利用领域知识提取域感知特征。
所提出的方法在频率上应用注意力模型，将领域知识嵌入情感表示空间。实验表明，我们的方法在IEMOCAP数据集上的表现达到了最先进的水平，在相同实验条件下加权准确率（WA）为73.02%，未加权准确率（UA）为65.86%。

###Chen22KEY-SPARSE
使用多模态输入可以进一步提高性能，因为使用了更多的情感信息。然而，这些多模态信息中，只有一小部分是关于情感的。冗余信息会成为噪声，限制系统性能。在本文中，我们提出了KS-Transformer，使用了一种新颖的关键稀疏注意力机制用于
语音情感识别。只有与情感相关的语音帧或文本中的词语会被考虑并分配注意力权重。基于KS-Transformer，我们进一步提出了CCAB来融合不同的模态并实现深度交互。在IEMOCAP和LSSED数据集上的实验结果证明了KS-Transformer和CCAB的有
效性。

###CHEAVD数据集-清华自动化实验室
CASIA中文自然情感视听数据库（CHEAVD），包含140分钟从电影、电视剧和脱口秀中提取的情感片段，238位年龄从儿童到老年人的说话者构成了广泛的说话者多样性。总共有26种非原型情感状态，包括基本的六种情感，由四位母语者进行标注。
与其他现有情感数据库相比，我们提供了多情感标签和虚假/抑制情感标签。该数据库是首个处理多模态和自然情感的大规模中文自然情感语料库，并免费供研究使用。我们在此语料库上使用长短期记忆循环神经网络（LSTM-RNN）进行了自动情感识别。
实验表明，在六种主要情感状态下，可以实现56%的平均准确率。




