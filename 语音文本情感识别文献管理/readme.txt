###语音情感识别综述_陶建华
本文从语音情感识别在国内外发展历史以及开展的一系列会议、期刊和竞赛入手，分别从6个方面对语音情感识别的研究现状进行了梳理与归纳。1.对情感表达从离散、维度模型进行了阐述；2.针对现有的情感数据库进行了统计与总结；3.回
顾了近20年部分代表性语音情感识别发展历程，并分别阐述了基于人工设计的语音情感特征的情感识别技术和基于端到端的语音情感识别技术；4.总结了近几年的语音情感识别性能，尤其是近两年在语音领域的重要会议和期刊上的语音情感
识别相关工作；5.介绍了语音情感识别在驾驶、智能交互领域、医疗健康，安全等领域的应用；6.总结与阐述了语音情感识别领域仍面临的挑战与未来发展方向。

###Liu22Group 
在多模态设置中，不同模态之间的时间对齐尚未得到充分研究。本文提出了一种名为门控双向对齐网络（GBAN）的新模型，该模型由一个基于注意力的双向对齐网络组成，通过LSTM隐藏状态明确捕捉语音和文本之间的对齐关系，以及一个
新颖的组门控融合（GGF）层来整合不同模态的表示。

###Li22Context-aware
提出了一种上下文感知多模态融合框架，包括一个基于上下文感知SSL的特征提取器和基于Transformer的音频-文本融合范式。首先使用预训练的WavLM和BERT模型，分别将原始音频和文本输入编码为帧级或标记级嵌入。然后，提取的高级
特征通过挤压和激励（S&E）模块校准和压缩，接着是一个全连接（FC）层和层归一化，这将两种模态映射到相同的空间。之后，采用一个上下文感知卷积模块来增强文本模态的目标话语的上下文信息。然后，我们提出了一种新颖的多模态
Transformer（MMT）融合模块来整合上述特征。MMT的关键组件是基于方向对跨模态注意力的Transformer（CMT），允许具有不同时间步的模态之间的交互。在融合模块之后，集成输出被送入情感分类器，取得了SOTA效果。

###Li21Learning（有代码）
考虑到情感表达复杂、多模态且细粒度。我们提出了一种新颖的多模态深度学习方法，用于从现实生活中的语音中进行细粒度的情感识别。我们设计了一种时间对齐均值-最大池化机制，以捕捉每个话语中隐含的细微和细粒度的情感。
此外，我们提出了一种跨模态激励模块，对跨模态嵌入进行样本特定的调整，并通过其对齐的另一模态的潜在特征自适应地重新校准相应的值。我们在两个著名的现实世界语音情感识别数据集上评估了我们提出的模型。结果表明，我们的方法在
多模态语音话语的预测任务上表现优越，并且在预测准确率方面优于多种基线方法。此外，我们进行了详细的消融研究，表明我们的时间对齐均-最大池化机制和跨模态激励显著贡献于令人满意的结果。
为了鼓励研究的可重复性，我们将代码公开在：https://github.com/tal-ai/FG_CME.git。

###Latiff20Unsupervised
SER系统的性能通常会因训练数据和测试数据分布的差异而下降。当训练数据和测试数据属于不同语言时，这些差异变得更加明显，导致验证分数和测试分数之间存在显著的性能差距。构建更为稳健的模型以适应实际应用中的SER系统是至关重要的。
本文提出了一种基于生成对抗网络（GAN）的多语言SER模型。我们选择使用GAN的原因在于其在学习数据分布方面取得的巨大成功。所提出的模型设计成能够学习语言不变的表示，而无需目标语言的数据标签。我们在四个不同语言的情感数据集上
评估了我们提出的模型，包括一个乌尔都语数据集，以纳入难以找到标注数据且主流社区研究较少的替代语言。我们的结果表明，所提出的模型可以显著提高所有考虑的数据集的基线跨语言SER性能，包括无需任何标签的非主流乌尔都语数据。

###Fan23Adaptive
当前，许多情感识别方法专注于域不变表示学习，这会丧失域特定的知识，导致跨域的语音情感识别效果不佳。本文提出了一种自适应域感知表示学习方法，该方法利用领域知识提取域感知特征。
所提出的方法在频率上应用注意力模型，将领域知识嵌入情感表示空间。实验表明，我们的方法在IEMOCAP数据集上的表现达到了最先进的水平，在相同实验条件下加权准确率（WA）为73.02%，未加权准确率（UA）为65.86%。

###Chen22KEY-SPARSE
使用多模态输入可以进一步提高性能，因为使用了更多的情感信息。然而，这些多模态信息中，只有一小部分是关于情感的。冗余信息会成为噪声，限制系统性能。在本文中，我们提出了KS-Transformer，使用了一种新颖的关键稀疏注意力机制用于
语音情感识别。只有与情感相关的语音帧或文本中的词语会被考虑并分配注意力权重。基于KS-Transformer，我们进一步提出了CCAB来融合不同的模态并实现深度交互。在IEMOCAP和LSSED数据集上的实验结果证明了KS-Transformer和CCAB的有
效性。

###CHEAVD数据集-清华自动化实验室
CASIA中文自然情感视听数据库（CHEAVD），包含140分钟从电影、电视剧和脱口秀中提取的情感片段，238位年龄从儿童到老年人的说话者构成了广泛的说话者多样性。总共有26种非原型情感状态，包括基本的六种情感，由四位母语者进行标注。
与其他现有情感数据库相比，我们提供了多情感标签和虚假/抑制情感标签。该数据库是首个处理多模态和自然情感的大规模中文自然情感语料库，并免费供研究使用。我们在此语料库上使用长短期记忆循环神经网络（LSTM-RNN）进行了自动情感识别。
实验表明，在六种主要情感状态下，可以实现56%的平均准确率。

###Latif20Deep
我们提出了一种更深层的神经网络架构，其中融合了DenseNet、LSTM和Highway Network，以学习强大的判别特征，这些特征对噪声具有较强的鲁棒性。我们还提出了在我们的网络架构中进行数据增强，以进一步提高鲁棒性。
我们对该架构结合数据增强在以下三种情况下进行了全面评估：(1) 噪声，(2) 对抗攻击，(3) 跨语料库设置。我们在广泛使用的IEMOCAP和MSP-IMPROV数据集上的评估显示，与现有研究和最先进模型相比，取得了有前途的结果。

###Rizos20STARGAN
许多人类具有在保持词汇结构不变的情况下操纵其情感表达的能力[19]。在这项研究中，我们利用StarGAN将真实的情感语音样本转换为不同的目标情感。通过这种方式，我们没有在信号上应用计算扰动（例如，如[20]中提出的或在[5]中添加抖动），而
是利用训练数据集中可用的情感建模信息，以尽可能保留语音内容和表达方式为原样，但改变所传达的情感。我们非常谨慎地评估生成样本的质量，并进行双重评估：a) 我们在多类情感计算任务中使用生成的样本进行数据增强；b) 我们对一组生成的样
本进行人类评估，以确定StarGAN是否真正学习了在样本中注入情感，而不是仅仅学会欺骗分类器（例如，由于模式崩溃）。为了保证可重复性，我们将在项目接受后，在项目的GitHub页面上提供所提出方法的实现，以确保原创性。
复现代码地址：https://github.com/glam-imperial/EmotionalConversionStarGAN


###SPEECH EMOTION RECOGNITION USING SEMANTIC INFORMATION
我们提出了一个使用音频和文本信息进行语音情感识别的训练框架。特别是，我们使用Word2Vec和Speech2Vec模型，并对其嵌入空间进行对齐，以仅使用语音信号进行准确的语义特征提取。我们使用一种新颖的注意力融合策略将语义和副语言特征结
合起来，该策略首先按情感维度分离信息，然后使用注意力进行结合。所提出的模型在SEWA数据集上进行了评估，并在效价和喜欢维度上与提交给AVEC 2017挑战赛的最佳论文相比，产生了最先进的结果。
代码地址：https://github.com/glam-imperial/semantic_speech_emotion_recognition?tab=readme-ov-file
SEWA数据集：使用了AVEC 2017挑战赛中使用的“野外情感分析”（SEWA）数据集。该数据集由通过网络摄像头和麦克风捕捉的32对（即64名参与者）的“野外”视听录音组成，这些参与者观看了一个90秒的商业广告，并与他们的伙伴讨论了最多3分钟。
该数据集提供了三种模态，即音频、视觉和文本，对三种情感维度进行标注：唤醒度、效价和喜欢。数据集分为三个部分：训练集（17对）、开发集（7对）和测试集（8对），并由6名德语标注员（3名女性，3名男性）进行标注。

###Upadhyay23Learning
不同的评价者对同一语音样本的情感感知可能会有所不同，这导致标注的情感标签存在主观性和模糊性。传统的SER模型通常采用所有评价者评分的平均值或共识标签进行训练，这可能会忽略不同评价者之间的主观差异。
本文通过聚类技术整合了评价者的主观性，形成感知一致性集群（PCC），将复杂的主观性问题简化为多数和少数评价者的感知聚类。
提出了一种基于评分者感知一致性的聚类方法，将评分者分为多数（MajP）和少数（MinP）两个群体，并基于此形成扩展的标签空间。
模型构建：构建了一个名为Rater Perception Coherency (RPC)-based SER的模型，该模型结合了多数和少数评分者的感知以及基于共识的情感分类器。
实验验证：在IEMOCAP和MSP-Podcast语料库上评估了提出的方法，考虑了固定和可变评分者的不同场景。
结果分析：通过实验，作者发现提出的RPC-based SER模型在UAR（Unweighted Average Recall）上分别比IEMOCAP和MSP-Podcast上的单任务模型提高了3.39%和2.03%。
代码地址：https://github.com/tal-ai/FG_CME


###Zhou23Emotion
在传统的情感语音转换（EVC）中，情感通常被视为离散的类别，忽略了语音还传达了听众可以感知到的不同强度的情感。在本文中，我们旨在明确表征和控制情感的强度。我们提出将说话者风格从语言内容中解耦，并将说话者风格编码为一个连续空间中的
风格嵌入，这构成了情感嵌入的原型。我们进一步从情感标注数据库中学习实际的情感编码器，并研究使用相对属性来表示细粒度的情感强度。为了确保情感的可理解性，我们在EVC网络的训练中加入了情感分类损失和情感嵌入相似性损失。正如预期的那样，
所提出的网络能够控制输出语音中的细粒度情感强度。通过客观和主观评估，我们验证了所提出网络在情感表达和情感强度控制方面的有效性。

代码地址（https://github.com/KunZhou9646/Emovox）

###Kreuk22Textless  meta Ai团队
在这篇论文中，研究人员把情感转换的问题作为一项口语翻译任务，将语音分解成离散的、不相干的，由内容单元、音调（f0）、说话人和情绪组成的学习表征。
模型先通过将内容单元翻译成目标情感来修改语音内容，然后根据这些单元来预测声音特征，最后通过将预测的表征送入一个神经声码器来生成语音波形。这种范式使得模型不止能发现信号的频谱和参数变化，还可以对非语言发声进行建模，如插入笑声、消除哈欠等。
比如在一个包含五种情绪表达方式（中立、愤怒、娱乐、困倦或厌恶）的情绪转换任务中，模型需要根据输入音频转换到目标情绪，可以看到整个流程就相当于是一个端到端的序列翻译问题，所以插入、删除、替换一些非语言的音频信号来转换情感就会更容易。
论文在客观上和主观上证明了所提出的方法在感知情感和音频质量方面优于基线。
代码地址：https://github.com/facebookresearch/fairseq/tree/main/examples/emotion_conversion

###Peng21Efficient（Efficient speech emotion recognition using multi-scale cnn and attention）
受启发于SWEM（Simple Word Embedding-Based Models），仅使用浅层特征就能达到深层特征的效果，且节约了内存空间，减少了复杂度。作者也使用了浅层的语音和文本特征来做文本+语音的情感分析，最终取得了很好的效果。特征融合方面，使用了多尺度
特征融合，实验中使用的Ground-truth transcript是人工标注的文本，而ASRtranscript是通过ASR转写的文本。实验结果显示，所提出的模型超过了SOTA大约3~4个点。从混淆矩阵可以看出大部分判断错误的类集中在中性类别上。
实际上这个现象比较合理，因为中性情感本身就位于不同情感的交界点上，所以对模型、甚至对人来说情感判断的难度比较大。
代码地址：https://github.com/KrishnaDN/Emotion-Recognition-Using-MSCNN-SPU


###MERC_Challenge_CCAC2023 代码和PPT汇报
第三届中国情感计算大会（The Third Chinese Conference on Affective Computing， CCAC 2023）将于2023年6月30日至7月2日在西安市举行，会议由西安交通大学承办。
本届多模态对话中的情感识别评测任务将采用M3ED（Multi-modal Multi-scene Multi-label Emotional Dialogue）数据集作为支撑数据集，该任务的任务描述、数据集描述以及评测描述如下。
任务描述：
本届多模态对话中的情感识别评测任务旨在识别M3ED数据集中不同语句（utterance）的情感。输入是含有文本、语音以及图像信息的对话，输出是对话中每条语句对应的情感。在本次评测中，
我们将对话中蕴含的情感分为以下七个类别之一：平静（Neutral）、开心（Happy）、惊讶（Surprised）、难过（Sad）、厌恶（Disgust）、生气（Angry）和害怕（Fear）。
说明：数据集原始标注是多情感标签标注（比如生气和伤心），为简化任务难度，本届测评采用单标签（主情感标签）情感识别。
数据集描述：
本次技术评测使用的标注数据集为M3ED数据集，由中国人民大学AI·M³多媒体计算实验室提供，原始数据源于56部中文电视剧的 990 个情感二人对话视频片段。M3ED数据集总共包含 24,449 个话语的标注，
存在丰富的情感互动。我们将发布视频片段所对应的视频信息、文本信息以及对应的标注信息，其中文本信息和标注信息将以json格式发布。
代码地址：https://github.com/AIM3-RUC/MERC_Challenge_CCAC2023?tab=readme-ov-file
还有这个网址，提供了M3ED数据集处理后的.pkl文件：https://github.com/AIM3-RUC/RUCM3ED?tab=readme-ov-file

###Liu23Multi-Level 
语音情感识别（SER）性能在噪声环境中显著下降，使得在嘈杂条件下实现竞争性表现变得具有挑战性。为此，我们提出了一种多层次知识蒸馏（MLKD）方法，旨在将从干净语音训练的教师模型中的知识转移到在嘈杂语音上训练的简化学生模型。
具体来说，我们使用由wav2vec-2.0提取的干净语音特征作为学习目标，训练蒸馏版的wav2vec-2.0在嘈杂条件下逼近原始wav2vec-2.0的特征提取能力。此外，我们利用原始wav2vec-2.0的多层次知识来监督蒸馏版wav2vec-2.0的单层输出。
我们通过在IEMOCAP数据集上进行大量使用五种类型噪声污染语音的实验，评估了我们提出方法的有效性，结果显示与最先进的模型相比取得了令人满意的结果。

###Sun23Fine-Grined
多模态情绪识别（MMER）是一个活跃的研究领域，旨在通过融合多种感知模式来准确识别人类情绪。然而，不同模式之间固有的异质性带来了分布差距和信息冗余，给MMER带来了重大挑战。
在本文中，我们提出了一种新的细粒度解纠缠表示学习（FDRL）框架来应对这些挑战。具体来说，我们设计了模态共享和模态私有编码器，将每个模态分别投射到模态共享和模态私有子空间中。
在共享子空间中，我们引入了一个细粒度的对齐组件来学习模态共享表示，从而捕获模态一致性。
随后，我们定制了一个细粒度的视差分量来约束私有子空间，从而学习模态私有表示并增强其多样性。
最后，我们引入了一个细粒度的预测器组件，以确保编码器输出表示的标签保持不变。IEMOCAP数据集的实验结果表明，FDRL在WAR和UAR上分别达到78.34%和79.44%。

###Muaz24Bridging（有代码）
本文解决了多模态情感识别理论进展与现实应用实际约束之间的差距，特别是将这些模型转化为更易获取和高效的单模态模型——仅使用语音进行情感识别。

在众多领域中，从语音中识别情感至关重要，包括人机界面、虚拟助手和心理健康评估。我们的目标是开发一个新框架，既保留多模态模型中丰富的理解，同时适应仅使用语音的情境约束和要求。
为实现这一目标，我们探索了两种独立的策略，利用知识蒸馏和掩蔽训练。知识蒸馏允许将细致的信息从最初在多模态输入上训练的教师模型转移到设计用于单模态、仅使用语音进行情感识别的精简学生模型。
同时，我们引入了掩蔽训练，这是一种旨在增强模型泛化到不完整输入能力的创新训练方法，同时提供来自其他模态的基础。此外，我们还研究了深度学习生成的音频特征嵌入对引入模型的影响。
通过一系列实验，我们评估了所提出框架在弥合多模态与单模态情感识别之间差距的有效性。结果不仅展示了多模态模型性能的提升，还证明了我们提出模型在仅使用语音进行情感识别这一挑战领域的竞争力。
本文的所有代码可以在以下链接找到：https://github.com/m-muaz/Cogmen_SLT

###IS CROSS-ATTENTIONPREFERABLETOSELF-ATTENTION FORMULTI-MODALEMOTIONRECOGNITION?
通常，融合多模态互补信息的模型比单模态模型表现更好。然而，一个成功的多模态融合模型需要能够有效聚合来自每个模态的任务相关信息的组件。
由于跨模态注意力被认为是多模态融合的有效机制，在本文中，我们量化了这种机制相对于相应的自注意力机制所带来的增益。为此，我们实现并比较了一个跨注意力和一个自注意力模型。
我们使用IEMOCAP数据集对基于自注意力和跨注意力的模型进行了三模态和双模态7类分类的比较。
结果表明，两种模型的结果之间没有显著差异。因此，在我们使用的数据集和架构的背景下，
我们得出结论：跨注意力在多模态情感识别中并不优于自注意力。此外，自注意力和跨注意力模型在识别任务中均改进了现有最先进水平。未来的工作包括研究跨注意力和自注意力模型在其他多模态任务和模态中的有效性。
复制实验的代码可以在https://github.com/smartcameras/SelfCrossAttn 找到。

#Wang23Exploring
最近的研究表明，自监督学习的声学和语言特征在这一任务中非常有用。然而，很少有工作充分利用了预训练特征在SER中的优势。主要的挑战在于如何有效地提取各个模态的预训练特征中所隐含的互补情感信息。
为了解决这个问题，我们提出了一种新颖的模态敏感多模态语音情感识别框架。简而言之，我们旨在利用每个模态中的典型情感特征，然后融合这些互补的情感信息进行分类。
具体来说，我们首先利用并行的单模态编码器从每个模态的预训练特征中提炼出与情感相关的信息。（后期融合）
为了更好地融合多模态特征，我们开发了一组可学习的情感查询令牌，通过Transformer解码器中的交叉注意力机制收集来自精炼后的声学和语言特征的情感信息。针对多模态方法中的模态偏差问题，我们引入了随机模态掩蔽训练策略，以最大化每个模态中的情感信息利用并缓解这一问题。
我们在广泛使用的IEMOCAP数据集上评估了我们的方法，并在未加权准确率和加权准确率上分别取得了1.1%和0.9%的改进。大量实验表明了所提出方法的有效性。

###Gan22DHF-Net（涉及意图）
为了在对话中识别特定情感时平衡上下文信息和细粒度信息的权衡，并结合层次特征相关信息的交互，本文提出了一种层次特征交互融合网络（称为DHF-Net），该网络不仅可以保留上下文序列信息的完整性，还可以提取更多细粒度信息。
为了获取深层语义信息，DHF-Net分别处理识别对话情感和对话行为/意图的任务，然后通过协作注意机制学习这两项任务的交叉影响。
此外，设计了一种双向门控循环单元（Bi-GRU）连接混合卷积神经网络（CNN）组的方法，通过该方法，序列信息被顺利传递到多层次局部信息层以进行特征提取。
实验结果表明，在两个公开会话数据集上，DHF-Net的性能分别提高了1.8%和1.2%。

###Min23Finding
通过将仇恨言论检测（HSD）与情感检测相结合来改进HSD，因为我们受到仇恨言论与某些负面情绪状态之间潜在关联的启发，这些关联已在理论和实证研究中得到了研究。具体来说，我们可以将仇恨标签和预测的情绪状态连接起来，作为仇恨言论样本的伪多标签，形成一个伪多标签学习（MLL）问题。
超越现有的多任务学习（MTL）-HSD方法，我们进一步将这种伪MLL问题纳入，并通过捕捉仇恨言论和负面情绪状态之间的关联来解决，以提高HSD的性能。基于这些想法，我们提出了一种新颖的HSD方法，称为情感关联仇恨言论检测器（EHSor）。
我们进行了广泛的实验来评估EHSor，结果表明，它在基准数据集上始终优于现有的HSD方法。
       受到仇恨言论与人类情感之间这种关联知识的启发，我们可以从多标签学习（MLL）的角度来看待HSD，其中每个训练样本同时分配仇恨标签和情感标签。我们可以通过捕捉仇恨和情感标签之间的关联来训练一个MLL分类器，从而生成增强情感的仇恨标签预测，以提高HSD的性能。然而，情感标签是未知的。
为了解决这个问题，我们在多任务学习（MTL）框架下将HSD与情感检测结合，并利用训练好的情感检测器生成伪情感标签。基于这些想法，我们提出了一种新颖的MTL-HSD方法，具有辅助情感检测，即情感关联仇恨言论检测器（EHSor）。
具体来说，EHSor由三个预测头组成，包括子仇恨检测器、情感检测器和超级仇恨检测器。子仇恨检测器和情感检测器通过共享的BERT编码器在标注的仇恨言论和辅助情感样本上联合训练。对于仇恨言论样本，我们可以分别使用子仇恨和情感检测器预测其仇恨标签和情感状态。
超级仇恨检测器在自训练范式中进行训练。具体来说，我们可以将仇恨预测与情感预测连接为潜在特征，将真实仇恨标签与锐化后的情感预测连接为伪多标签，从而形成伪MLL数据。超级仇恨检测器通过使用相关模块在伪MLL数据上进行训练，该模块可以捕捉伪多标签之间的关联。
最终，EHSor使仇恨言论和人类情感不仅可以从低共享层进行交换，还可以通过多标签学习从更高层次的语义交互中进行交换。EHSor可以以端到端的方式进行训练，超级仇恨检测器最终用于预测未见的仇恨言论样本。我们进行了广泛的实验。结果表明，EHSor始终优于现有的HSD基线方法。
       本文的贡献可以概括为三点：
       1. 我们通过综合考虑现有的心理学研究和我们自己的实验结果，验证了仇恨言论与某些负面情绪状态之间的关联。
       2. 受到关联发现的启发，我们提出了一种新颖的HSD方法，称为EHSor，它从MLL的角度联合训练HSD和辅助情感检测。
       3. 我们在基准数据集上进行了广泛的实验。结果表明，EHSor优于现有的HSD方法，特别是那些与辅助情感检测问题联合训练的方法。

###Liu22Emerging（多标签学习综述）
极端多标签分类是一个活跃且快速增长的研究领域，处理具有极大量类或标签的分类任务；利用有限监督的大量数据来构建多标签分类模型对于实际应用变得非常有价值。除此之外，还有大量研究致力于如何利用深度学习的强大学习能力来更好地捕捉多标签学习中的标签依赖性，这是深度学习解决现实世界分类任务的关键。
然而，值得注意的是，目前缺乏专门分析大数据时代多标签学习新趋势和新挑战的系统性研究。迫切需要进行一次全面的综述，以完成这一任务，并描绘未来的研究方向和新应用。


###Morais19Deciding（多标签假新闻）
在这项工作中，我们讨论了新闻文档的客观性和合法性之间的区别，将每篇文章视为具有两个概念类别：客观/讽刺和合法/假。因此，我们提出了一种基于文本挖掘管道和一组新颖文本特征的决策支持系统（DSS），该系统使用多标签方法来对这两个领域的新闻文章进行分类。
为了验证这种方法，我们评估了一组多标签方法，并结合了不同的基础分类器，然后与多类别方法进行了比较。结果表明，我们的DSS在处理误导性新闻方面是合适的（F1-score为0.80），在具有挑战性的多标签建模视角中优于多类别方法（F1-score为0.71），这些数据来自多个新闻门户收集的现实生活新闻数据集。
     这项工作的次要贡献包括：
     (1) 提出我们的现实生活多标签新闻数据集；
     (2) 提出新的文本特征（超出词汇表特征、摘要减少率和每段平均词数）；
     (3) 确定在我们的多标签新闻场景中最佳的机器学习算法和文本特征。

###Catelli22Deceptive （情感极性做欺骗检测，使用多标签分类方案对情感感知的欺骗性评论检测系统进行建模，自动提取有利于通过情感信息识别评论欺骗性的隐藏特征）
Kennedy等人（2019）和Kim等人（2021）利用BERT执行欺骗性评论识别；类似的Martinez-Torres和Toral（2019）的研究，展示了如何通过适当的特征工程，将情感极性信息匹配为特征，来使用经典分类器进行欺骗性评论识别。
就目前所知，迄今为止还没有人尝试结合这两种方法的优势：我们的贡献正是朝这个方向努力。具体而言，本文章的提议包括以下贡献：1.开发了一个基于预训练上下文语言模型的欺骗性评论检测系统，特别是测试了BERT和DistilBERT模型在区分大小写和不区分大小写版本中的表现；
2.将情感极性分类整合到上述系统中，利用提到的语言模型通过词嵌入捕捉词语的多义性，并通过检测评论中常见的同义词和反义词（如fantastic, great, horrible, terrible等）有效地表示句法-语义关系；
3.通过基于标签幂集（Label Powerset, LP）技术的多标签分类方案对情感感知的欺骗性评论检测系统进行建模，使用[欺骗性，情感极性]作为幂集：在微调阶段利用构成语言模型的深度神经网络，自动提取有利于通过情感信息识别评论欺骗性的隐藏特征；
在一个小规模的数据集，即DOSC数据集，和一个中等规模的数据集，即YelpNYC数据集上，进行了实验、分析并与文献中现有的最新研究进行了比较。

###Firdaus20MEISD（多标签情感强度对话数据集）
在对话中进行情感和情绪分类是一项具有挑战性的任务，近年来受到了广泛关注。人类在表达思想和感受时往往会表现出多种情感，并具有不同的强度。在对话中的一个话语中的情感可以是独立的，也可以依赖于之前的话语，使得这一任务既复杂又有趣。
对话中的多标签情感检测是一项重要任务，它使系统能够理解用户在互动中表现出的各种情感。另一方面，对对话或谈话中的情绪分析有助于理解用户对于正在进行的对话的看法。除了文本信息外，音频和视频形式的附加信息也有助于识别对话中话语的正确情感及其相应的强度和情绪。
最近，已经有不少数据集用于对话中的情感和情绪分类。然而，这些数据集在不同情感的表示上不平衡，并且仅包含单一情感。
因此，我们首先提出了一个大型平衡的多模态多标签情感、强度和情绪对话数据集（MEISD），该数据集从不同的电视剧中收集，包含文本、音频和视觉特征，并建立了一个基准设置供进一步研究。

###Zhou20Multi-Classifier （语音情感多标签分类，模糊建模-情感概率分布）
由于整个语音中的情感可能具有不同的概率，语音情感往往具有模糊性，这对识别任务提出了巨大挑战。然而，先前的研究通常为每个语音片段分配单一标签或多标签。因此，它们的算法由于表示方式不合适而导致准确率较低。
受到最佳交互理论的启发，我们提出了一种新的多分类器交互学习（MCIL）方法来解决模糊的语音情感问题。在MCIL中，多个不同的分类器首先模拟一些对模糊情感有不同认知的个体，并构建新的模糊标签（情感概率分布）。
然后，它们用新的标签进行再训练，以便与彼此的认知进行交互。这个过程使每个分类器能够从其他分类器中学习更好的模糊数据表示，进一步提高了识别能力。在三个基准语料库（MAS、IEMOCAP和FAU-AIBO）上的实验表明，MCIL不仅提高了每个分类器的性能，还将它们的识别一致性从中等提高到显著。

###velichko22Complex（语音情感和欺骗相关的文章）
使用情感进行欺骗检测的总体思路基于心理学和副语言学的研究。根据四因素理论[12]，欺骗包括影响人类行为的不同心理过程和条件。该模型中揭示欺骗者的关键因素是：情感反应、激活、认知努力和行为控制的尝试。与欺骗相关的情感状态引发无法控制的行为变化，这些变化可以在不同的非语言渠道中检测到[13]。
著名心理学家Paul Ekman[14]建议了三种与欺骗相关的主要情感：恐惧、羞愧和欺骗的愉悦感。Amiriparian等人[15]探讨了高级特征生成（激活、效价、调节和情感类别）的使用。Mendels等人[16]提出了另一种使用情感和词汇特征进行欺骗检测的模型。
然而，迄今为止，还没有研究以整体方式分析和预测性别、情感和欺骗。我们的工作首次提出了一个复杂的副语言系统，通过结合性别和情感信息，强调这些现象之间的联系，改进了语音欺骗检测。

###Mittal20Emotions（情绪不会说谎，通过视听情绪不一致来检测深度伪造）



